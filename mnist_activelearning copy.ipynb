{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mai\\anaconda3\\envs\\bmc\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data :  60000\n",
      "number of test data :  10000\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.MNIST(root = './data/02/',\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST(root = './data/02/',\n",
    "                            train=False,\n",
    "                            download=True,\n",
    "                            transform=transforms.ToTensor())\n",
    "print('number of training data : ', len(train_data))\n",
    "print('number of test data : ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = read(\"training\", path=\"./mnist\")\n",
    "show(mnist[n 번째 데이터][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data/02/\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.to(device))\n",
    "        loss = loss_fn(pred.to('cpu'), y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.to(device)).to('cpu')\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306836  [    0/60000]\n",
      "loss: 2.301640  [ 6400/60000]\n",
      "loss: 2.296018  [12800/60000]\n",
      "loss: 2.289612  [19200/60000]\n",
      "loss: 2.282989  [25600/60000]\n",
      "loss: 2.283163  [32000/60000]\n",
      "loss: 2.284438  [38400/60000]\n",
      "loss: 2.277572  [44800/60000]\n",
      "loss: 2.273957  [51200/60000]\n",
      "loss: 2.259861  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 2.259657 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.262303  [    0/60000]\n",
      "loss: 2.254974  [ 6400/60000]\n",
      "loss: 2.258946  [12800/60000]\n",
      "loss: 2.243677  [19200/60000]\n",
      "loss: 2.226478  [25600/60000]\n",
      "loss: 2.238956  [32000/60000]\n",
      "loss: 2.233966  [38400/60000]\n",
      "loss: 2.231411  [44800/60000]\n",
      "loss: 2.196052  [51200/60000]\n",
      "loss: 2.202729  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: 2.198227 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.217794  [    0/60000]\n",
      "loss: 2.194479  [ 6400/60000]\n",
      "loss: 2.177497  [12800/60000]\n",
      "loss: 2.178800  [19200/60000]\n",
      "loss: 2.154648  [25600/60000]\n",
      "loss: 2.153645  [32000/60000]\n",
      "loss: 2.144289  [38400/60000]\n",
      "loss: 2.134222  [44800/60000]\n",
      "loss: 2.118337  [51200/60000]\n",
      "loss: 2.089761  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 2.094013 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.111748  [    0/60000]\n",
      "loss: 2.088589  [ 6400/60000]\n",
      "loss: 2.029984  [12800/60000]\n",
      "loss: 2.085582  [19200/60000]\n",
      "loss: 2.087264  [25600/60000]\n",
      "loss: 2.026881  [32000/60000]\n",
      "loss: 1.960166  [38400/60000]\n",
      "loss: 2.061472  [44800/60000]\n",
      "loss: 1.936225  [51200/60000]\n",
      "loss: 1.898809  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 1.914287 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.941479  [    0/60000]\n",
      "loss: 1.904559  [ 6400/60000]\n",
      "loss: 1.861300  [12800/60000]\n",
      "loss: 1.828645  [19200/60000]\n",
      "loss: 1.842482  [25600/60000]\n",
      "loss: 1.774872  [32000/60000]\n",
      "loss: 1.739584  [38400/60000]\n",
      "loss: 1.794838  [44800/60000]\n",
      "loss: 1.723920  [51200/60000]\n",
      "loss: 1.690888  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 1.640042 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.630199  [    0/60000]\n",
      "loss: 1.603814  [ 6400/60000]\n",
      "loss: 1.652169  [12800/60000]\n",
      "loss: 1.525648  [19200/60000]\n",
      "loss: 1.479690  [25600/60000]\n",
      "loss: 1.545251  [32000/60000]\n",
      "loss: 1.401928  [38400/60000]\n",
      "loss: 1.300336  [44800/60000]\n",
      "loss: 1.389037  [51200/60000]\n",
      "loss: 1.409195  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 1.322698 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.446054  [    0/60000]\n",
      "loss: 1.380197  [ 6400/60000]\n",
      "loss: 1.270067  [12800/60000]\n",
      "loss: 1.189681  [19200/60000]\n",
      "loss: 1.230019  [25600/60000]\n",
      "loss: 1.336050  [32000/60000]\n",
      "loss: 1.076465  [38400/60000]\n",
      "loss: 1.135221  [44800/60000]\n",
      "loss: 1.035770  [51200/60000]\n",
      "loss: 1.064452  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 1.058190 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.101112  [    0/60000]\n",
      "loss: 0.961774  [ 6400/60000]\n",
      "loss: 0.990155  [12800/60000]\n",
      "loss: 1.113702  [19200/60000]\n",
      "loss: 1.098167  [25600/60000]\n",
      "loss: 1.022210  [32000/60000]\n",
      "loss: 0.951774  [38400/60000]\n",
      "loss: 0.957132  [44800/60000]\n",
      "loss: 0.839373  [51200/60000]\n",
      "loss: 0.829832  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.874077 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.962613  [    0/60000]\n",
      "loss: 0.879779  [ 6400/60000]\n",
      "loss: 0.945792  [12800/60000]\n",
      "loss: 0.831576  [19200/60000]\n",
      "loss: 0.922792  [25600/60000]\n",
      "loss: 0.761385  [32000/60000]\n",
      "loss: 0.807547  [38400/60000]\n",
      "loss: 0.811865  [44800/60000]\n",
      "loss: 0.825133  [51200/60000]\n",
      "loss: 0.750131  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.748658 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.943793  [    0/60000]\n",
      "loss: 0.890545  [ 6400/60000]\n",
      "loss: 0.675458  [12800/60000]\n",
      "loss: 0.727481  [19200/60000]\n",
      "loss: 0.796290  [25600/60000]\n",
      "loss: 0.751733  [32000/60000]\n",
      "loss: 0.609418  [38400/60000]\n",
      "loss: 0.618430  [44800/60000]\n",
      "loss: 0.738235  [51200/60000]\n",
      "loss: 0.772066  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.661523 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mai\\anaconda3\\envs\\bmc\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")\n",
    "# import pickle\n",
    "# with open('data.p', 'wb') as f:\n",
    "#     pickle.dump(mnist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data.p\", 'rb') as f:\n",
    "    mnist = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = mnist[\"data\"]\n",
    "Data[\"target\"] = mnist[\"target\"]\n",
    "\n",
    "Data = Data.sample(frac=1)\n",
    "Data = Data.sample(frac=1).reset_index(drop=True)  # shuffling하고 index reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image = Data.iloc[:,:-1].to_numpy().reshape(-1,28,28)\n",
    "Label = Data.iloc[:,-1:].astype(int).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image = torch.from_numpy(Image)\n",
    "Label = torch.from_numpy(Label).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, image, label, transform=None):\n",
    "        self.img = image\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        digit = self.img[idx].reshape(28,28)\n",
    "        if self.transform:\n",
    "            digit = self.transform(digit)\n",
    "        return digit, self.label[idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(TensorDataset):\n",
    "    def __init__(self, image, label, transform=None):\n",
    "        self.img = image\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        digit = Image[idx].reshape(28,28)\n",
    "        if self.transform:\n",
    "            digit = self.transform(digit)\n",
    "        return digit, Label[idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2 = MnistDataset(Image[:10], Label[:], transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCustomBatch:\n",
    "    def __init__(self, data):\n",
    "        transposed_data = list(zip(*data))\n",
    "        self.inp = torch.stack(transposed_data[0], 0)\n",
    "        self.tgt = torch.stack(transposed_data[1], 0)\n",
    "\n",
    "    # custom memory pinning method on custom type\n",
    "    def pin_memory(self):\n",
    "        self.inp = self.inp.pin_memory()\n",
    "        self.tgt = self.tgt.pin_memory()\n",
    "        return self\n",
    "\n",
    "def collate_wrapper(batch):\n",
    "    return SimpleCustomBatch(batch)\n",
    "\n",
    "inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "dataset = TensorDataset(inps, tgts)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n",
    "                    pin_memory=True)\n",
    "\n",
    "for batch_ndx, sample in enumerate(loader):\n",
    "    print(sample.inp.is_pinned())\n",
    "    print(sample.tgt.is_pinned())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features2, train_labels2 = next(iter(train_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader1 = DataLoader(TensorDataset(Image, Label), batch_size=64,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader2 = DataLoader(train_data2, batch_size=4,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader1))\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mai\\Desktop\\Active Learning\\mnist_activelearning.ipynb 셀 31\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_loop(train_dataloader1, model, loss_fn, optimizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\mai\\Desktop\\Active Learning\\mnist_activelearning.ipynb 셀 31\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(X\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m), y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mai\\anaconda3\\envs\\bmc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\mai\\Desktop\\Active Learning\\mnist_activelearning.ipynb 셀 31\u001b[0m in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_relu_stack(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#Y111sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32mc:\\Users\\mai\\anaconda3\\envs\\bmc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mai\\anaconda3\\envs\\bmc\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mai\\anaconda3\\envs\\bmc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mai\\anaconda3\\envs\\bmc\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader1, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_full = x_train[:4000] / 255\n",
    "y_full = y_train[:4000]\n",
    "x_test = x_test[:400] /255\n",
    "y_test = y_test[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "1          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "2          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "3          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...  ...       ...       ...       ...       ...       ...   \n",
       "69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0           0.0       0.0       0.0       0.0       0.0  \n",
       "1           0.0       0.0       0.0       0.0       0.0  \n",
       "2           0.0       0.0       0.0       0.0       0.0  \n",
       "3           0.0       0.0       0.0       0.0       0.0  \n",
       "4           0.0       0.0       0.0       0.0       0.0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "69995       0.0       0.0       0.0       0.0       0.0  \n",
       "69996       0.0       0.0       0.0       0.0       0.0  \n",
       "69997       0.0       0.0       0.0       0.0       0.0  \n",
       "69998       0.0       0.0       0.0       0.0       0.0  \n",
       "69999       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[70000 rows x 784 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5'], dtype=object)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int32' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mai\\Desktop\\Active Learning\\mnist_activelearning.ipynb 셀 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mai/Desktop/Active%20Learning/mnist_activelearning.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandperm(X\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int32' object is not callable"
     ]
    }
   ],
   "source": [
    "indices = torch.randperm(X.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = \n",
    "test_set = X[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21c43c38820>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bmc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f61ad4722c2a8c82edf5a6e472f62a3dc3ef4a65ca69f027a39c1c64bc3f4c70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
